当我们已经训练好一个模型时，需要把模型保存下来，这样训练的模型才能够随时随地的加载与使用。模型的保存和加载分为两部分，一部分是模型的定义即图的存取，另一部分中图中的变量的存取（常量存储在图中）。Tensorflow可以分别完成两部分的内容。

##1. 图存取##

保存图的方法如下：

~~~python
with tf.Graph().as_default() as graph:
    v = tf.Variable([1, 2])

with tf.Session(graph=graph) as sess:
        tf.train.write_graph(sess.graph_def, './model', 'model.pbtxt')

~~~

获取图的方法：

~~~python
with tf.Session() as sess:
    with tf.gfile.FastGFile('./model/model.pbtxt', 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        sess.graph.as_default()
        tf.import_graph_def(graph_def, name='mygraph')
~~~

`tf.import_graph_def()`将一个`GraphDef`导入当前的默认图中。

##2. 变量存取##

变量存储是把模型中定义的变量存储起来，不包含图结构。另一个程序使用时，首先需要重新创建图，然后将存储的变量导入进来，即模型加载。变量存储可以脱离图存储而存在。

变量的存储与读取，在Tensorflow中叫做检查点存取，变量保存的文件是检查点文件(checkpoint file)，扩展名一般为.ckpt。使用`tf.train.Saver()`类来操作检查点。

###2.1 存入###

变量存储实际上是存储会话中的变量，所以需要在会话中保存变量。这里需要注意的是`tf.train.Saver()`类的初始化位置很重要，`tf.train.Saver()`初始化之前的变量将会被保存，之后的变量不会被保存。通常，我们可以再图定义完成之后初始化`tf.train.Saver()`。

保存变量的方法是`tf.train.Saver.save()`，这里我们需要给方法传入变量保存地址，变量所在会话等信息。

如下：

~~~python
import tensorflow as tf

with tf.Graph().as_default() as default_graph:
    x = tf.placeholder(tf.float32, shape=[3, ])
    var1 = tf.Variable([1., 2., 3.])
    add_op = tf.add(x, var1)
    assign_op = var1.assign_add(add_op)
    saver = tf.train.Saver()

with tf.Session(graph=default_graph) as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(assign_op, feed_dict={x: [1, 2, 3]})  # [ 3.  6.  9.]
    sess.run(assign_op, feed_dict={x: [1, 2, 3]})  # [  7.  14.  21.]
    saver.save(sess, './log/model.ckpt')
~~~

上述代码初始化了一个变量，之后运行了一些计算，最后保存了变量，变量的值应该为`[  7.  14.  21.]`。变量保存的地址是`./log/model.ckpt`。这时候我们打开`./log`文件夹，可以看到有四个文件，分别是checkpoint文件、model.ckpt.data-00000-of-00001文件、model.ckpt.index文件、model.ckpt.meta文件。这四个文件的作用分别是：

* checkpoint：保存当前模型的位置与最近的5个模型的信息。这里我们只保存了一个模型，所有只有一个模型信息，也是当前模型的信息。
* model.ckpt.data-00000-of-00001：保存了当前模型变量的数据。
* model.ckpt.meta：保存了`MetaGraphDef`，可以用于恢复saver对象。
* model.ckpt.index：辅助model.ckpt.meta的数据文件。

使用saver存储训练时的变量数据是很方便的。当我们训练一个较复杂的模型时，可能需要很长的时间，我们可以设置每训练一定次数就保存一次模型，避免因为断电或意外错误使训练的结果付之东流。这时候在保存变量时，需要将训练次数设置到saver的`global_step`参数上，例如：

~~~python
import tensorflow as tf

with tf.Graph().as_default() as default_graph:
    x = tf.placeholder(tf.float32, shape=[3, ])
    var1 = tf.Variable([1., 2., 3.])
    add_op = tf.add(x, var1)
    assign_op = var1.assign_add(add_op)
    saver = tf.train.Saver()

with tf.Session(graph=default_graph) as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(1, 100):
        print(sess.run(assign_op, feed_dict={x: [1, 1, 1]}))
        if i % 10 == 0:
            saver.save(sess, './log/model.ckpt', global_step=i)
~~~

###2.2 读取###

变量读取，即加载模型数据，为了保证数据能够正确加载，必须首先将图定义好，而且必须与保存时的图定义一致。这里“一致”的意思是图相同，对于Python句柄等Python环境中的内容可以不同。

下面我们恢复上文中保存的图：

~~~python
import tensorflow as tf

with tf.Graph().as_default() as default_graph2:
    y = tf.placeholder(tf.float32, shape=[3, ])
    var = tf.Variable([1., 2., 3.])
    add_op = tf.add(y, var)
    assign_op = var.assign_add(add_op)
    saver = tf.train.Saver()

with tf.Session(graph=default_graph2) as sess2:
    sess2.run(tf.global_variables_initializer())
    saver.restore(sess2, './log/model.ckpt')
    print(sess2.run(var))
~~~

上述代码中我们将句柄`x`和`var1`分布替换成为了`y`和`var`，但并没有改变模型定义，所以上述代码是正确的。上述代码使用了`saver.restore()`方法还原了变量，这个方法必须填入当前回话以及模型位置。

恢复变量时我们还可以借助meta文件，如下：

~~~python
import tensorflow as tf

with tf.Graph().as_default() as default_graph2:
    y = tf.placeholder(tf.float32, shape=[3, ])
    var = tf.Variable([1., 2., 3.])
    add_op = tf.add(y, var)
    assign_op = var.assign_add(add_op)
    # saver = tf.train.Saver()

with tf.Session(graph=default_graph2) as sess2:
    sess2.run(tf.global_variables_initializer())
    saver = tf.train.import_meta_graph('./log/model.ckpt.meta')
    saver.restore(sess2, './log/model.ckpt')
    print(sess2.run(var))
~~~

当我们循环存储变量时，检查点文件的名称会发生变化，显然手动设置是很繁琐的，这时候我们可以利用`tf.train.get_checkpoint_state()`自动获取当前最新的检查点。

`tf.train.get_checkpoint_state()`方法通过传入检查点所在目录，就可以获取检查点信息。所有上述代码盖伊改进成为：

~~~python
import tensorflow as tf

with tf.Graph().as_default() as default_graph2:
    y = tf.placeholder(tf.float32, shape=[3, ], name='p')
    var = tf.Variable([1., 2., 3.])
    add_op = tf.add(y, var)
    assign_op = var.assign_add(add_op)
    saver = tf.train.Saver()

with tf.Session(graph=default_graph2) as sess2:
    sess2.run(tf.global_variables_initializer())
    ckpt = tf.train.get_checkpoint_state('./log')
    saver.restore(sess2, ckpt.model_checkpoint_path)
    print(sess2.run(var))
~~~

