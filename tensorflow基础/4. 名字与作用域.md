Tensorflow的Tensor、Variable、Operation均拥有自己的名字name，用来区分变量。由于Tensorflow是符号式编程，我们必须指定一个名字来使C++或别的接口找到Tensor或op。Tensorflow也有作用域(scope)，用来管理变量、op等。Tensorflow的作用域分为两种，一种是variable_scope，一种是name_scope。简言之，variable_scope主要给variable_name加前缀，也可以给op_name加前缀；name_scope是给op_name加前缀。

##name

在Tensorflow中，所有的Tensor与Operation均有一个`name`属性，是Tensor与Operation的标识。`name`是一个可选参数，可以创建带`name`或者不带`name`的Tensor与Operation。**`name`的命名规范是可以为数字、字母、下划线，不能以下划线开始**。如下：

~~~python
# 给常量指定name
const = tf.constant([1, 2, 3], name='const')
# <tf.Tensor 'const:0' shape=(3,) dtype=int32>
print(const.name) # >> const:0

# 给变量指定name
var = tf.Variable([1, 2, 3], name='var')
# <tf.Variable 'var:0' shape=(3,) dtype=int32>
print(var.name) # >> var:0

# 给2个op指定相同name
op1 = tf.multiply(1, 2, name='op')
# <tf.Tensor 'op:0' shape=() dtype=int32>
print(op1.name) # >> op:0
op2 = tf.divide(1, 2, name='op')
# <tf.Tensor 'op_1:0' shape=() dtype=int32>
print(op2.name)  # >> op_1:0

# 不指定op的name
op3 = tf.add(1, 2)
# <tf.Tensor 'Add:0' shape=() dtype=int32>
print(op3.name)  # >> Add:0
~~~

###name构成

Tensor的`name`由三部分构成，(I)是op产生的名称，也就是我们指定的name；(II)是冒号；(III)是op输出内容的索引，默认从0开始。Operation的`name`没有冒号与索引，例如：

~~~python
var = tf.Variable([1, 2, 3], name='var')
var_op = var.initializer
# <tf.Operation 'var/Assign' type=Assign>
print(var_op.name)  # var/Assign
~~~

这里`name`中包含一个反斜杠，是作用域符号。即这个Op的name表示了其在作用域var下面。

###通过name获得Tensor

我们可以通过`tf.Graph.get_tensor_by_name()`方法，利用name查询到对应的Tensor。例如：

```python
const = tf.constant([1, 2, 3], name='const') # name = 'const:0'
const_tensor = tf.get_default_graph().get_tensor_by_name('const:0')
print(const_tensor)  # >> Tensor("const:0", shape=(3,), dtype=int32)
```



## name_scope

name_scope可以用来给op_name加前缀。其目的是区分功能模块，可以更方便的在TensorBoard中可视化，同时也便于管理name，以及便于持久化和重新加载。

name_scope使用`tf.name_scope()`创建，返回一个上下文管理器。name_scope的参数`name`可以是字母、数字、下划线，不能以下划线开头。类似于变量的参数`name`的命名。

~~~python
a = tf.constant(1, name='const')
print(a.name)  # >> const:0

with tf.name_scope('my_name') as name:
  	print(name)  # >> my_name/
  	b = tf.constant(1, name='const')
    print(b.name)  # >> my_name/const:0
~~~

name_scope在当前作用域下不允许重名，当我们指定name相同的两个或多个name_scope时，系统会自动为name_scope增加后缀。如下：

~~~python
with tf.name_scope('my_name') as name1:
  	print(name1)  # >> my_name/
with tf.name_scope('my_name') as name2:
  	print(name2)  #>> my_name_1/
~~~

###多级name_scope

name_scope可以嵌套，嵌套之后的name包含上级的name，如下：

~~~python
with tf.name_scope('name1'):
  	with tf.name_scope('name2') as name2:
      	print(name2)  # >> name1/name2/
~~~

在多级name_scope中，op的name会被加上一个前缀，这个前缀取决于所在的name_scope。不同级中的name可以因为其前缀不同，所以不可能重名，如下：

~~~python
with tf.name_scope('name1'):
  	a = tf.constant(1, name='const')
    print(a.name)  # >> name1/const:0
  	with tf.name_scope('name2'):
      	b = tf.constant(1, name='const')
    	print(b.name)  # >> name1/name2/const:0
~~~

### name_scope的作用范围

使用name_scope可以给op_name加前缀，包括使用Variable()创建的变量。但不会影响使用`tf.get_variable()`创建的变量，如下所示：

~~~python
with tf.name_scope('name'):
  	var = tf.Variable([1, 2], name='var')
    print(var.name)  # >> name/var:0
    var2 = tf.get_variable(name='var2', shape=[2, ])
    print(var2.name)  # >> var2:0
~~~



##variable_scope

variable_scope也可以用来给name加前缀，包括variable_name与op_name都可以。与name_scope相比，variable_scope功能要更丰富，最重要的是其可以给get_variable()创建的变量加前缀。

variable_scope使用`tf.variable_scope()`创建，返回一个上下文管理器。name_scope的参数`name`可以是字母、数字、下划线，不能以下划线开头。类似于变量的参数`name`以及name_scope的命名。

###给op_name加前缀

variable_scope包含了name_scope的功能，即可以给op_name加前缀，如下：

~~~python
with tf.variable_scope('my_scope') as scope:
  	# 使用scope.name查看其variable_scope的name
    # 其包含了name_scope，但查到的并不是name_scope的name，这两者是不同
  	print(scope.name)  # >> my_scope
  
    a = tf.constant([1, 2, 3], name='const')
    print(a.name)  # my_scope/const:0
    
    var = tf.Variable([1, 2, 3], name='var')
    print(var.name)  # my_scope/var:0
    
    var_op = var.initializer
    print(var_op.name)  # my_scope/var/Assign
~~~

### 给variable_name加前缀

variable_scope与name_scope最大的不同就是，variable_scope可以给使用`tf.get_variable()`创建的变量加前缀，如下：

~~~python
with tf.variable_scope('my_scope'):
  	op = tf.get_variable('var', shape=[2, ])
    print(op.name)  # >> my_scope/var:0
~~~



###同名variable_scope

variable_scope可以是同名的，同名的variable_scope中不允许通过`tf.get_variable`创建相同name的Variable。下面的代码会抛出一个ValueError的错误：

~~~python
with tf.variable_scope('s1'):
  	var1 = tf.get_variable('var')

with tf.variable_scope('s1'):
  	# 抛出错误
    # 相同的variable_scope中不允许出现使用
    # tf.get_variable()创建的同名的Variable
  	var2 = tf.get_variable('var')
~~~



同名variable_scope中variable_scope的name是一样的，name_scope的name是不一样。如下：

~~~python
with tf.variable_scope('my_scope') as scope1:
  	# 输出variable_scope的name 注意 不是name_scope的name
  	print(scope1.name)  # >> my_scope
    print(tf.constant(1, name='const').name)  # >> my_scope/const:0

with tf.variable_scope('my_scope') as scope2:
  	print(scope2.name)  # >> my_scope
    print(tf.constant(1, name='const').name)  # >> my_scope_1/const:0
~~~



使用一个variable_scope初始化另一个variable_scope。这两个variable_scope的name一样，name_scope的name不一样。等价于两个同名的variable_scope。如下：

~~~python
with tf.variable_scope('my_scope') as scope1:
  	print(scope1.name)  # >> my_scope
    
with tf.variable_scope(scope1) as scope2:
 	print(scope2.name)  # >> my_scope
    print(tf.constant(1, name='const').nname)  # >> my_scope_1/const:0
~~~



####variable_scope的reuse参数

当我们将variable_scope的reuse参数设置为`True`时，配合`tf.get_variable()`可以共享同名的variable_scope中变量。这时候`tf.get_variable()`的作用是获取同名的其他variable_scope下的变量，而不能用来创建变量，如果使用`tf.get_variable()`创建变量会抛出ValueError的错误。如下：

~~~python
with tf.variable_scope('my_scope') as scope1:
  	# 默认情况下reuse=None
    # 可以使用tf.get_variable()创建变量
    # 创建变量时，不可以创建相同name的变量
  	var = tf.get_variable('var', shape=[2, 3])

with tf.variable_scope('my_scope', reuse=True) as scope2:
  	# 当reuse=False或者None时
    # 可以使用tf.get_variable()获取变量
    # 可以多次执行tf.get_variable()获取变量
  	var2= tf.get_variable('var')
    
print(var is var2)  # >> True
~~~



### 多级变量作用域

我们可以再一个作用域中，使用另一个作用域，这时候作用域中的name也会叠加在一起，如下：

~~~python
with tf.variable_scope('first_scope') as first_scope:
  	print(first_scope.name)  # >> first_scope
  	with tf.variable_scope('second_scope') as second_scope:
      	print(second_scope.name)  # >> first_scope/second_scope
        print(tf.get_variable('var', shape=[1, 2]).name)  
        # >> first_scope/second_scope/var:0
~~~

####跳过作用域

如果在开启的一个变量作用域里使用之前预定义的一个作用域，则会跳过当前变量的作用域，保持预先存在的作用域不变：

~~~python
with tf.variable_scope('outside_scope') as outside_scope:
  	print(outside_scope.name)  # >> outside_scope

with tf.variable_scope('first_scope') as first_scope:
  	print(first_scope.name)  # >> first_scope
  	print(tf.get_variable('var', shape=[1, 2]).name)  
    # >> first_scope/var:0
  	with tf.variable_scope(outside_scope) as second_scope:
      	print(second_scope.name)  # >> outside_scope
      	print(tf.get_variable('var', shape=[1, 2]).name) 
        # >> outside_scope/var:0
~~~

#### 多级变量作用域中的reuse

在多级变量作用域中，规定外层的变量作用域设置了`reuse=True`，内层的所有作用域的`reuse`必须设置为`True`（设置为其它无用）。

多级变量作用域中，使用`tf.get_variable()`的方法如下：

~~~python
# 定义
with tf.variable_scope('s1') as s1:
    tf.get_variable('var', shape=[1,2])
    with tf.variable_scope('s2') as s2:
        tf.get_variable('var', shape=[1,2])
        with tf.variable_scope('s3') as s3:
            tf.get_variable('var', shape=[1,2])

# 使用
with tf.variable_scope('s1', reuse=True) as s1:
    v1 = tf.get_variable('var')
    with tf.variable_scope('s2', reuse=None) as s2:
        v2 = tf.get_variable('var')
        with tf.variable_scope('s3', reuse=None) as s3:
            v3 = tf.get_variable('var')
~~~



### 变量作用域的初始化

variable_scope可以在创建是携带一个初始化器。其作用是将在其中的变量自动使用初始化器的方法进行初始化。方法如下：

~~~python
# 直接使用tf.get_variable()得到的是随机数
var1 = tf.get_variable('var1', shape=[3, ])
var1.eval()
# 输出的可能值：
# array([-0.92183685, -0.078825  , -0.61367416], dtype=float32)
~~~

~~~python
# 使用variable_scope的初始化器
with tf.variable_scope(
  					 'scope', 
  					 initializer=tf.constant_initializer(1)):
  	var2 = tf.get_variable('var2', shape=[3, ])
	var1.eval()  # 输出 [ 1.  1.  1.]
~~~

常见的初始化器有：

~~~python
# 常数初始化器
tf.constant_initializer(value=0, dtype=tf.float32)
# 服从正态分布的随机数初始化器
tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=None, dtype=tf.float32)
# 服从截断正态分布的随机数初始化器
tf.truncated_normal_initializer(mean=0.0, stddev=1.0, seed=None, dtype=tf.float32)
# 服从均匀分布的随机数初始化器
tf.random_uniform_initializer(minval=0, maxval=None, seed=None, dtype=tf.float32)
# 全0初始化器
tf.zeros_initializer(dtype=tf.float32)
# 全1初始化器
tf.ones_initializer(dtype=tf.float32)
~~~

